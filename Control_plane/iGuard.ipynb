{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XT_8EM4CPHoE",
        "outputId": "70dee76d-d467-4c6a-bb27-d3a00f3bfbcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/HorusEye"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FOaYYHuPL6O",
        "outputId": "14f42353-96c0-4100-86f2-8bf4be3259ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/HorusEye\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report,accuracy_score\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score, average_precision_score\n",
        "from sklearn.metrics import f1_score\n",
        "from fcntl import F_SETFL\n",
        "import pickle\n",
        "# from re import T\n",
        "import time\n",
        "import torch\n",
        "import torch.utils.data as Data\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import roc_curve, auc, precision_recall_curve, classification_report\n",
        "# from thop import clever_format\n",
        "# from thop import profile\n",
        "from torch import nn, optim\n",
        "import random\n",
        "from queue import Queue\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "criterion = nn.MSELoss()\n",
        "scaler = preprocessing.MinMaxScaler()"
      ],
      "metadata": {
        "id": "vrlgL-e6PPQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Autoencoder Model"
      ],
      "metadata": {
        "id": "9dmKOfZGCAX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DilatedSeparableConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, dilation):\n",
        "        super(DilatedSeparableConv, self).__init__()\n",
        "        self.depthwise = nn.Conv1d(in_channels, in_channels, kernel_size=3, padding=dilation, dilation=dilation, groups=in_channels)\n",
        "        self.pointwise = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x\n",
        "\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        torch.manual_seed(42)\n",
        "        torch.cuda.manual_seed(42)\n",
        "        self.encoder = nn.Sequential(\n",
        "            DilatedSeparableConv(1, 16, dilation=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool1d(2),\n",
        "            DilatedSeparableConv(16, 32, dilation=2),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool1d(2),\n",
        "            DilatedSeparableConv(32, 64, dilation=4),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool1d(2),\n",
        "            nn.Conv1d(64, latent_dim, kernel_size=1)\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose1d(latent_dim, 64, kernel_size=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose1d(64, 32, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose1d(32, 16, kernel_size=5, stride=2, padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose1d(16, 1, kernel_size=6, stride=2, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "jtD7SsTtPhFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #new autoencoder to try that 4 attacks for better f1\n",
        "# class DilatedSeparableConv(nn.Module):\n",
        "#     def __init__(self, in_channels, out_channels, dilation):\n",
        "#         super(DilatedSeparableConv, self).__init__()\n",
        "#         self.depthwise = nn.Conv1d(in_channels, in_channels, kernel_size=3, padding=dilation, dilation=dilation, groups=in_channels)\n",
        "#         self.pointwise = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.depthwise(x)\n",
        "#         x = self.pointwise(x)\n",
        "#         return x\n",
        "\n",
        "# class Autoencoder(nn.Module):\n",
        "#     def __init__(self, input_dim, latent_dim):\n",
        "#         super(Autoencoder, self).__init__()\n",
        "#         torch.manual_seed(42)\n",
        "#         torch.cuda.manual_seed(42)\n",
        "#         # Encoder\n",
        "#         self.encoder = nn.Sequential(\n",
        "#             DilatedSeparableConv(1, 16, dilation=1),\n",
        "#             nn.ReLU(True),\n",
        "#             DilatedSeparableConv(16, 32, dilation=2),\n",
        "#             nn.ReLU(True),\n",
        "#             nn.MaxPool1d(2),\n",
        "#             DilatedSeparableConv(32, 64, dilation=4),\n",
        "#             nn.ReLU(True),\n",
        "#             nn.MaxPool1d(2),\n",
        "#             DilatedSeparableConv(64, 128, dilation=8),  # Additional layer\n",
        "#             nn.ReLU(True),\n",
        "#             nn.MaxPool1d(2),\n",
        "#             nn.Conv1d(128, latent_dim, kernel_size=1)  # Reducing to latent dimension\n",
        "#         )\n",
        "\n",
        "#         # Decoder\n",
        "#         self.decoder = nn.Sequential(\n",
        "#             nn.ConvTranspose1d(latent_dim, 128, kernel_size=1),\n",
        "#             nn.ReLU(True),\n",
        "#             nn.ConvTranspose1d(128, 64, kernel_size=5, stride=2, padding=1),\n",
        "#             nn.ReLU(True),\n",
        "#             nn.ConvTranspose1d(64, 32, kernel_size=5, stride=2, padding=1),\n",
        "#             nn.ReLU(True),\n",
        "#             nn.ConvTranspose1d(32, 16, kernel_size=5, stride=1, padding=1),\n",
        "#             nn.ReLU(True),\n",
        "#             nn.ConvTranspose1d(16, 1, kernel_size=6, stride=1, padding=1),\n",
        "#             nn.Sigmoid()  # Assuming input is normalized to [0, 1]\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.encoder(x)\n",
        "#         x = self.decoder(x)\n",
        "#         return x\n"
      ],
      "metadata": {
        "id": "1UtNiAsrm6YW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Autoencoder"
      ],
      "metadata": {
        "id": "Z9Goq137CEAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_autoencoder(df_normal_train, df_normal_eval, df_attack_eval):\n",
        "    X_train = df_normal_train.values\n",
        "    X_test = pd.concat([df_normal_eval, df_attack_eval]).values\n",
        "    actual = torch.cat([torch.zeros(df_normal_eval.shape[0]), torch.ones(df_attack_eval.shape[0])])\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    input_dim = X_train.shape[1]\n",
        "    encoding_dim = 8\n",
        "    num_epochs = 1000\n",
        "    batch_size = 512\n",
        "\n",
        "    autoencoder = Autoencoder(input_dim, encoding_dim)\n",
        "    criterion = nn.BCELoss()\n",
        "    #optimizer = optim.SGD(autoencoder.parameters(), lr=0.1, weight_decay=0.001)\n",
        "    optimizer = optim.Adam(autoencoder.parameters(), lr=1e-3)\n",
        "    #optimizer = optim.RMSprop(autoencoder.parameters(), lr=0.001, alpha=0.9)\n",
        "    #optimizer = optim.Adagrad(autoencoder.parameters(), lr=0.01)\n",
        "\n",
        "    autoencoder = autoencoder.cuda()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for i in range(0, len(X_train), batch_size):\n",
        "            inputs = torch.tensor(X_train[i:i+batch_size], dtype=torch.float).unsqueeze(0)\n",
        "            inputs = inputs.cuda()\n",
        "            inputs = inputs.transpose(0, 1)\n",
        "            outputs = autoencoder(inputs)\n",
        "            loss = criterion(outputs, inputs)\n",
        "            total_loss += loss\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if (epoch+1) % 200 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss.sum():.4f}')\n",
        "\n",
        "    autoencoder.eval()\n",
        "    with torch.no_grad():\n",
        "        input = torch.tensor(X_test, dtype=torch.float).unsqueeze(0).transpose(0, 1).cuda()\n",
        "        decoded_data = autoencoder(input).cpu().numpy()\n",
        "    decoded_data_binary = decoded_data.squeeze()\n",
        "    mse = np.mean(np.power(X_test - decoded_data_binary, 2), axis=1)\n",
        "    rmse = np.sqrt(mse)\n",
        "    # print(rmse)\n",
        "    # print(np.quantile(rmse,(1-0.2)))\n",
        "    for i in [0.001]: #play around threshold\n",
        "        thres = i\n",
        "        print(\"-------------i\",i)\n",
        "        predicted = pd.Series(np.where(rmse > thres, 1, 0),dtype=\"float64\")\n",
        "        f1 = f1_score(actual, predicted)\n",
        "        print(confusion_matrix(actual, predicted))\n",
        "        print(\"\\n Classification report\")\n",
        "        print(classification_report(actual, predicted))\n",
        "        print('F1 Score: ', f1)\n",
        "\n",
        "    return autoencoder"
      ],
      "metadata": {
        "id": "-TURqyAZ1MBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Isolation Forest from scratch"
      ],
      "metadata": {
        "id": "T0QAhwW6CKwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "    def __init__(self, data, left=None, right=None, depth=0):\n",
        "        self.data = data\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.depth = depth\n",
        "        self.label = None\n",
        "        self.val = None\n",
        "        self.col = None\n",
        "\n",
        "class isolationTree:\n",
        "    rmse_scores = []\n",
        "    def __init__(self, data, depth=0, max_depth=0, thres = None):\n",
        "        self.max_depth = max_depth\n",
        "        self.thres = thres\n",
        "        self.root = self.build_tree(data, depth)\n",
        "        self.model = None\n",
        "        self.num_rows = 50\n",
        "\n",
        "    def build_tree(self, data, depth):\n",
        "        if len(data) == 0:\n",
        "            return None\n",
        "        elif len(data) == 1:\n",
        "            return Node(data, depth=depth)\n",
        "        else:\n",
        "            q = Queue()\n",
        "            root = Node(data, depth=depth)\n",
        "            q.put(root)\n",
        "\n",
        "            while not q.empty():\n",
        "                current_node = q.get()\n",
        "                if len(current_node.data) > 1:\n",
        "                  split_column = random.randint(0, len(current_node.data.iloc[0]) - 1)\n",
        "                  i = 0\n",
        "                  while True:\n",
        "                    if min(current_node.data.iloc[:, split_column]) != max(current_node.data.iloc[:, split_column]):\n",
        "                      break\n",
        "                    i += 1\n",
        "                    if i == 9:\n",
        "                      break\n",
        "                    # print(f\"Number of points: {len(current_node.data)}, {split_column}\")\n",
        "                    split_column = (split_column + 1) % len(current_node.data.iloc[0])\n",
        "                  if i == 9:\n",
        "                    continue\n",
        "                  split_value = random.uniform(min(current_node.data.iloc[:, split_column]), max(current_node.data.iloc[:, split_column]))\n",
        "                  left_data = current_node.data[current_node.data.iloc[:, split_column] <= split_value]\n",
        "                  right_data = current_node.data[current_node.data.iloc[:, split_column] > split_value]\n",
        "                  current_node.val = split_value\n",
        "                  current_node.col = split_column\n",
        "\n",
        "                  if len(left_data) > 0:\n",
        "                      left_node = Node(left_data, depth=current_node.depth + 1)\n",
        "                      current_node.left = left_node\n",
        "                      if len(left_data) != 1:\n",
        "                        q.put(left_node)\n",
        "\n",
        "                  if len(right_data) > 0:\n",
        "                      right_node = Node(right_data, depth=current_node.depth + 1)\n",
        "                      current_node.right = right_node\n",
        "                      if len(right_data) != 1:\n",
        "                        q.put(right_node)\n",
        "            return root\n",
        "\n",
        "    def label_tree(self, root):\n",
        "\n",
        "        if root.left is None and root.right is None:\n",
        "            sampled_df = root.data.sample(n=self.num_rows, replace=True, random_state=42)\n",
        "\n",
        "            noise = np.random.normal(0, 0.01, sampled_df.shape)\n",
        "            sampled_df = scaler.transform(sampled_df)\n",
        "            sampled_df = sampled_df + noise\n",
        "            sampled_df = torch.tensor(sampled_df, dtype=torch.float32).unsqueeze(0).transpose(0,1)\n",
        "            pred = self.model(sampled_df)\n",
        "            rmse = criterion(pred, sampled_df)\n",
        "            rmse = torch.sqrt(rmse)\n",
        "            print(rmse.item())\n",
        "            isolationTree.rmse_scores.append(rmse.item())\n",
        "            root.label = 1 if rmse > self.thres else 0\n",
        "            # print(rmse, root.label)\n",
        "        else:\n",
        "            if root.left:\n",
        "              self.label_tree(root.left)\n",
        "            if root.right:\n",
        "              self.label_tree(root.right)\n",
        "\n",
        "    def pred_tree(self, data, node):\n",
        "      if node is None:\n",
        "        return 1, 0\n",
        "\n",
        "      if node.left is None and node.right is None:\n",
        "        return node.label, 0\n",
        "      if data[node.col] <= node.val:\n",
        "        label, path_len = self.pred_tree(data,node.left)\n",
        "        return label, 1 + path_len\n",
        "      if data[node.col] > node.val:\n",
        "        label, path_len = self.pred_tree(data,node.right)\n",
        "        return label, 1 + path_len\n",
        "\n",
        "\n",
        "class isolationForest(nn.Module):\n",
        "    def __init__(self, data, n_trees=100, max_depth=5, subspace=256, model=None):\n",
        "        super().__init__()\n",
        "        self.data = data\n",
        "        self.n_trees = n_trees\n",
        "        self.max_depth = max_depth\n",
        "        self.subspace = subspace\n",
        "        self.model = model\n",
        "        self.thres = 0.022\n",
        "        self.avg_path_len = 2 * (np.log(subspace - 1) + 0.5772) - 2 * (subspace - 1) / subspace\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self):\n",
        "        for i in range(self.n_trees):\n",
        "            if self.subspace > 1:\n",
        "                subdata = self.data.sample(self.subspace)\n",
        "            else:\n",
        "                subdata = self.data.sample(frac=self.subspace)\n",
        "            tree = isolationTree(subdata, depth=0, max_depth = self.max_depth, thres = self.thres)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    # def anomaly_score(self, x):\n",
        "    #     scores = []\n",
        "    #     for tree in self.trees:\n",
        "    #         score = self.path_length(tree, x)\n",
        "    #         scores.append(score)\n",
        "    #     avg_score = np.mean(scores)\n",
        "    #     return avg_score\n",
        "\n",
        "    # def path_length(self, node, x):\n",
        "    #     if node is None:\n",
        "    #         return 0\n",
        "    #     elif np.all(x == node.data):\n",
        "    #         return 0\n",
        "    #     else:\n",
        "    #         left_path_len = self.path_length(node.left, x)\n",
        "    #         right_path_len = self.path_length(node.right, x)\n",
        "    #         path_len = 1 + (left_path_len if left_path_len > right_path_len else right_path_len)\n",
        "    #         return path_len\n",
        "\n",
        "    def label(self, model):\n",
        "        for tree in self.trees:\n",
        "            tree.model = model\n",
        "        for tree in self.trees:\n",
        "            tree.label_tree(tree.root)\n",
        "            print(\"--------------predicted threshold : \", np.quantile(isolationTree.rmse_scores,(1-0.2)))\n",
        "\n",
        "    def pred(self, data, alpha):\n",
        "      eval = []\n",
        "      for i,row in data.iterrows():\n",
        "          temp = [tree.pred_tree(row, tree.root) for tree in self.trees]\n",
        "          res = min(temp)[1], max(temp)[1]\n",
        "          agg_label = sum(x[0] for x in temp) / self.n_trees\n",
        "          avg_len = sum(x[1] for x in temp) / self.n_trees\n",
        "\n",
        "          iso_score = 2 ** (- avg_len / self.avg_path_len)\n",
        "          print(f\"len: {avg_len} score: {iso_score}\")\n",
        "          eval.append(-1 if alpha * agg_label + (1 - alpha) * iso_score > 0.5 else 1)\n",
        "      return eval\n"
      ],
      "metadata": {
        "id": "1N-2Lg1hP0l5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pred(model, data, alpha):\n",
        "  eval = []\n",
        "  for i,row in data.iterrows():\n",
        "      temp = [tree.pred_tree(row, tree.root) for tree in model.trees]\n",
        "      res = min(temp)[1], max(temp)[1]\n",
        "      agg_label = sum(x[0] for x in temp) / model.n_trees\n",
        "      avg_len = sum(x[1] for x in temp) / model.n_trees\n",
        "\n",
        "      iso_score = 1 - 2 ** (- avg_len / model.avg_path_len - np.log(1-0.1475) / np.log(2))\n",
        "      # print(f\"len: {avg_len} score: {iso_score}\")\n",
        "      eval.append(-1 if alpha * agg_label + (1 - alpha) * iso_score > 0.5 else 1)\n",
        "  return eval"
      ],
      "metadata": {
        "id": "Y49o8BXcP77e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training our isolation forest model"
      ],
      "metadata": {
        "id": "CUGDfd6bCOmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def isolation_train(x_train,attack_x_train):\n",
        "    tmp = pd.concat([x_train,attack_x_train])\n",
        "    tmp.fillna(0,inplace=True)\n",
        "    tmp.columns = range(len(tmp.columns))\n",
        "    clf_model = isolationForest(tmp, n_trees=50, max_depth=8, subspace=200)\n",
        "    clf_model.fit()\n",
        "    print(\"Isolation Forest training completed\\n\")\n",
        "    return clf_model"
      ],
      "metadata": {
        "id": "_WqCqyUYP8f6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training sklearn's isolation forest model"
      ],
      "metadata": {
        "id": "6Ov_8kSYCTB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sk_iforest(a):\n",
        "    sklearn_clf_model = IsolationForest(n_estimators=50, max_samples=200, random_state=114514,\n",
        "                             contamination=0.2,n_jobs=8)\n",
        "    tmp = pd.concat([x_train,attack_x_train])\n",
        "    tmp.fillna(0,inplace=True)\n",
        "    sklearn_clf_model.fit(tmp)\n",
        "    y_pred_eval = sklearn_clf_model.predict(x_eval)\n",
        "    eval_y = y_eval\n",
        "    eval_x = x_eval\n",
        "\n",
        "    y_pred_eval[y_pred_eval == 1] = 0\n",
        "    y_pred_eval[y_pred_eval == -1] = 1\n",
        "    temp_str = classification_report(y_true=eval_y, y_pred=y_pred_eval)\n",
        "    temp_list = temp_str.split()\n",
        "    print(\"sklearn Isolation Forest\")\n",
        "    print(temp_str)\n",
        "    print(\"Confusion Matrix\")\n",
        "    print(confusion_matrix(eval_y, y_pred_eval))\n",
        "    print(\"\\n F1 Score\")\n",
        "    print(f1_score(eval_y, y_pred_eval))\n",
        "    print(\"\\n roc-auc\")\n",
        "    print(roc_auc_score(eval_y, y_pred_eval))\n",
        "    print(\"\\n pr-auc\")\n",
        "    print(average_precision_score(eval_y, y_pred_eval))\n",
        "    name = 'iForest'+a+'.pkl'\n",
        "    with open(name, 'wb') as f:\n",
        "        pickle.dump(sklearn_clf_model, f)"
      ],
      "metadata": {
        "id": "PDJjo_GDQWuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Executing our training autoencoder function"
      ],
      "metadata": {
        "id": "RIAR_qboCb71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def autoencode():\n",
        "    torch.manual_seed(42)\n",
        "    torch.cuda.manual_seed(42)\n",
        "    auto = train_autoencoder(df_norm_train.iloc[:,:12], df_normal_eval.iloc[:,:12], df_attack_eval.iloc[:,:12])\n",
        "    print(\"Autoencoder training completed\\n\")\n",
        "    # with open('autoencoder.pkl', 'wb') as f:\n",
        "    #     pickle.dump(auto, f)\n",
        "    return auto"
      ],
      "metadata": {
        "id": "_vc6VnHAQHQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Executing our training model function"
      ],
      "metadata": {
        "id": "yhFLcSJHCpWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def our_model(auto,a):\n",
        "    clf_model = isolation_train(x_train,attack_x_train)\n",
        "    auto = auto.cpu()\n",
        "    clf_model.label(auto)\n",
        "    print(\"Isolation Forest labeling completed\\n\")\n",
        "    x_eval = eval\n",
        "    x_eval.fillna(0,inplace=True)\n",
        "    x_eval.columns = range(len(x_eval.columns))\n",
        "    y_pred_eval = pred(clf_model, x_eval, 1)\n",
        "    eval_y = y_eval\n",
        "    eval_x = x_eval\n",
        "\n",
        "    y_pred_eval = np.array(y_pred_eval)\n",
        "    y_pred_eval[y_pred_eval == 1] = 0\n",
        "    y_pred_eval[y_pred_eval == -1] = 1\n",
        "    print(\"Our Isolation Forest + Autoencoder\")\n",
        "    print(\"Confusion Matrix\")\n",
        "    print(confusion_matrix(eval_y, y_pred_eval))\n",
        "    print(\"\\n Classification report\")\n",
        "    print(classification_report(eval_y, y_pred_eval))\n",
        "    print(\"\\n F1 Score\")\n",
        "    print(f1_score(eval_y, y_pred_eval))\n",
        "    print(\"\\n roc-auc\")\n",
        "    print(roc_auc_score(eval_y, y_pred_eval))\n",
        "    print(\"\\n pr-auc\")\n",
        "    print(average_precision_score(eval_y, y_pred_eval))\n",
        "    name = 'iGuard'+a+'.pkl'\n",
        "    with open(name, 'wb') as f:\n",
        "        pickle.dump(clf_model, f)"
      ],
      "metadata": {
        "id": "1mXFnh_yQIy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Block"
      ],
      "metadata": {
        "id": "zrUcchq6B3PZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device_list=['360_camera']\n",
        "attack_list=['http_ddos','data_theft','keylogging','service_scan','tcp_ddos','mirai','os_scan','aidra','bashlite','mirai_router_filter','os_scan_router','port_scan_router','tcp_ddos_router','udp_ddos','udp_ddos_router']\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "contamination = 0.2\n",
        "for a in attack_list:\n",
        "\n",
        "    normal_path = \"/content/drive/MyDrive/HorusEye/DataSets/Dataplane/Normal/\" + device_list[0] + \".csv\"\n",
        "    attack_path = \"/content/drive/MyDrive/HorusEye/DataSets/Dataplane/Anomaly/\" + a + \".csv\"\n",
        "    df_normal_train_data = pd.read_csv(normal_path)\n",
        "    df_attack_train = pd.read_csv(attack_path)\n",
        "    df_normal_train_data = df_normal_train_data.drop(columns=['tcp_udp','dst_port','n_packets'])\n",
        "    df_attack_train = df_attack_train.drop(columns=['tcp_udp','dst_port','n_packets'])\n",
        "    df_normal_train_data = df_normal_train_data.applymap(int)\n",
        "    df_attack_train = df_attack_train.applymap(int)\n",
        "    datafetch = True\n",
        "    n1 = 200\n",
        "    n2 = 1000\n",
        "    while(datafetch):\n",
        "        if contamination != -1:\n",
        "            try:\n",
        "                num = int((n1 / contamination - n1) / 0.2)\n",
        "                temp = df_normal_train_data.sample(n=num, replace=False, random_state=20)\n",
        "                df_norm_train = df_normal_train_data.drop(temp.index)\n",
        "\n",
        "                df_attack_train = df_attack_train.sample(n=n2, replace=False, random_state=20)\n",
        "                df_normal_train_data = temp\n",
        "                datafetch = False\n",
        "            except ValueError as e:\n",
        "                if \"Cannot take a larger sample than population when 'replace=False'\" in str(e):\n",
        "                    n1 = int(n1/2)\n",
        "                    n2 = int(n2/2)\n",
        "                else:\n",
        "                    raise e\n",
        "\n",
        "    df_normal_train, df_normal_eval = train_test_split(df_normal_train_data, test_size=0.2, random_state=20)\n",
        "    df_norm_train = pd.concat([df_norm_train, df_normal_train])\n",
        "    df_attack_train, df_attack_eval = train_test_split(df_attack_train, test_size=0.2, random_state=20)\n",
        "    x_train, y_train = df_normal_train.drop(columns=['class']), df_normal_train['class']\n",
        "    attack_x_train, attack_y_train = df_attack_train.drop(columns=['class']), df_attack_train['class']\n",
        "    df_eval = pd.concat([df_normal_eval, df_attack_eval])\n",
        "    x_eval, y_eval = df_eval.drop(columns=['class']), df_eval['class']\n",
        "    eval = x_eval\n",
        "    print(\"------------------------------Attack : \",a, \"--------------------------------\\n\")\n",
        "    sk_iforest(a)\n",
        "    #use this to train autoencoder also\n",
        "    # no = int(0.01 * len(df_normal_train))\n",
        "    # selected_samples = df_attack_train.sample(no, replace=False)\n",
        "    # df_normal_train = df_normal_train.loc[selected_samples.index] = selected_samples\n",
        "    auto = autoencode()\n",
        "    # torch.save(auto.state_dict(), 'autoencoder_model.pth')\n",
        "    # use this to load pretrained autoencoder\n",
        "    # with open('autoencoder_final.pkl', 'rb') as f:\n",
        "    #     auto = pickle.load(f)\n",
        "    # auto = Autoencoder(12, 8)\n",
        "    # auto.load_state_dict(torch.load('auto_85.pth'))\n",
        "    # scaler.fit(df_normal_train.iloc[:,:12])\n",
        "    our_model(auto,a)"
      ],
      "metadata": {
        "id": "tG_YODU_QLaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optional testing trained autoencoder model"
      ],
      "metadata": {
        "id": "oWEa31hbBrcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function just to test the autoencoder\n",
        "def test_ae(autoencoder):\n",
        "    X_test = pd.concat([df_normal_eval.iloc[:,:12], df_attack_eval.iloc[:,:12]]).values\n",
        "    actual = torch.cat([torch.zeros(df_normal_eval.shape[0]), torch.ones(df_attack_eval.shape[0])])\n",
        "\n",
        "    X_test = scaler.transform(X_test)\n",
        "    autoencoder.eval()\n",
        "    with torch.no_grad():\n",
        "        input = torch.tensor(X_test, dtype=torch.float).unsqueeze(0).transpose(0, 1)\n",
        "        decoded_data = autoencoder(input).cpu().numpy()\n",
        "    decoded_data_binary = decoded_data.squeeze()\n",
        "    mse = np.mean(np.power(X_test - decoded_data_binary, 2), axis=1)\n",
        "    rmse = np.sqrt(mse)\n",
        "    # print(rmse)\n",
        "    for i in [0.35]:\n",
        "        thres = i\n",
        "        print(\"-------------i\",i)\n",
        "        predicted = pd.Series(np.where(rmse > thres, 1, 0),dtype=\"float64\")\n",
        "        f1 = f1_score(actual, predicted)\n",
        "        print(confusion_matrix(actual, predicted))\n",
        "        print(\"\\n Classification report\")\n",
        "        print(classification_report(actual, predicted))\n",
        "        print('F1 Score: ', f1)"
      ],
      "metadata": {
        "id": "ebpTyagzw4nE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#To generate dataset from .pcap files"
      ],
      "metadata": {
        "id": "k1lCbo8yBaWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scapy"
      ],
      "metadata": {
        "id": "bfk-CfS4BcmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scapy.all import *\n",
        "import os\n",
        "import numpy as np\n",
        "import pickle\n",
        "import csv\n",
        "from math import sqrt\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def file_name_list(file_dir):\n",
        "    file_list = []\n",
        "    for root, dirs, files in os.walk(file_dir):\n",
        "        for file in files:\n",
        "            if os.path.splitext(file)[1] == \".pcap\":\n",
        "                file_list.append(\"{}/{}\".format(root, file))\n",
        "    return file_list\n",
        "\n",
        "#features = [\"n_packets\", \"size_total\", \"size_avg\", \"size_var\", \"size_std\", \"ipd_av\", \"ipd_min\", \"ipd_var\", \"ipd_std\", \"class\"]\n",
        "features = [\"n_packets\", \"size_total\", \"size_avg\", \"size_var\", \"size_std\", \"size_min\", \"size_max\", \"ipd_av\", \"ipd_min\", \"ipd_var\", \"ipd_std\", \"ipd_max\", \"flow_dur\", \"tcp_udp\",\"dst_port\",\"class\"]\n",
        "\n",
        "if not os.path.exists(\"/content/drive/MyDrive/HorusEye/DataSets/Dataplane/Normal\"):\n",
        "    os.makedirs(\"/content/drive/MyDrive/HorusEye/DataSets/Dataplane/Normal\")\n",
        "if not os.path.exists(\"/content/drive/MyDrive/HorusEye/DataSets/Dataplane/Anomaly\"):\n",
        "    os.makedirs(\"/content/drive/MyDrive/HorusEye/DataSets/Dataplane/Anomaly\")\n",
        "\n",
        "\n",
        "\n",
        "idle_timeout = 0.01\n",
        "pkt_thres = 3\n",
        "# FIN = 0x01\n",
        "# RST = 0x04\n",
        "\n",
        "fls_train = 0\n",
        "fls_test = 0\n",
        "\n",
        "def extract_features(pkt_list):\n",
        "    feats = [0, 0, 0, 0, 0, 9999999999999, 0, 0, 9999999999,0,0,0,0,0,0]\n",
        "    #n_packets\n",
        "    feats[0] = len(pkt_list)\n",
        "    #size_total\n",
        "    feats[1] = sum([len(pkt) for pkt in pkt_list])\n",
        "    #size_avg\n",
        "    feats[2] = sum([len(pkt) for pkt in pkt_list])/len(pkt_list)\n",
        "    #size_var\n",
        "    feats[3] = sum([(len(pkt) - feats[2]) ** 2 for pkt in pkt_list]) / len(pkt_list)\n",
        "    #size_std\n",
        "    feats[4] = sqrt(feats[3])\n",
        "    #size_min\n",
        "    feats[5] = min([len(pkt) for pkt in pkt_list])\n",
        "    #size_max\n",
        "    feats[6] = max([len(pkt) for pkt in pkt_list])\n",
        "\n",
        "    #ipd_av\n",
        "    feats[7] = (pkt_list[-1].time - pkt_list[0].time)/(len(pkt_list)) #IPD_AV\n",
        "    for i in range(len(pkt_list) - 1):\n",
        "        #ipd_min\n",
        "        feats[8] = min(pkt_list[i+1].time - pkt_list[i].time, feats[8]) #IPD MIN\n",
        "        #ipd_var\n",
        "        feats[9] = (abs(pkt_list[i+1].time - pkt_list[i].time - feats[7]))**2 + feats[9]\n",
        "        feats[11] = max(pkt_list[i+1].time - pkt_list[i].time, feats[9])\n",
        "    feats[9] = feats[7]/(len(pkt_list))\n",
        "    feats[10] = sqrt(feats[9])\n",
        "    feats[12] = (pkt_list[-1].time - pkt_list[0].time)\n",
        "    if TCP in pkt_list[0]:\n",
        "        feats[-1] = pkt_list[0][TCP].dport\n",
        "        feats[-2] = 6\n",
        "    elif UDP in pkt_list[0]:\n",
        "        feats[-1] = pkt_list[0][UDP].dport\n",
        "        feats[-2] = 17\n",
        "\n",
        "    return feats\n",
        "\n",
        "def extract_flows(path, label,name):\n",
        "    global fls_test, fls_train, idle_timeout,X,y, pkt_thres\n",
        "    X = []\n",
        "    y = []\n",
        "    c = 0\n",
        "    lst = file_name_list(path)\n",
        "    print(lst)\n",
        "    for f in lst:\n",
        "        flows = {}\n",
        "        c += 1\n",
        "        print(c)\n",
        "        packets = PcapReader(os.path.join(path, f))\n",
        "        for packet in packets:\n",
        "            if IP in packet and (TCP in packet or UDP in packet):\n",
        "                # Define a tuple that represents the 5-tuple information of the packe\n",
        "                if TCP in packet:\n",
        "                    flow_tuple = (packet[IP].src, packet[IP].dst, packet[TCP].sport, packet[TCP].dport, 'TCP')\n",
        "                else:\n",
        "                    flow_tuple = (packet[IP].src, packet[IP].dst, packet[UDP].sport, packet[UDP].dport,'UDP')\n",
        "\n",
        "                if(flow_tuple in flows):\n",
        "                    # if((len(flows[flow_tuple]) >= pkt_thres) and (packet.time - flows[flow_tuple][-1].time > idle_timeout or  (TCP in packet and (packet['TCP'].flags & FIN or packet['TCP'].flags & RST)))):\n",
        "                    if((len(flows[flow_tuple]) >= pkt_thres) or (packet.time - flows[flow_tuple][-1].time > idle_timeout or  (TCP in packet and (packet['TCP'].flags or packet['TCP'].flags)))):\n",
        "                        X.append(extract_features(flows[flow_tuple]))\n",
        "                        y.append(label)\n",
        "                        flows[flow_tuple] = [packet]\n",
        "                    elif(len(flows[flow_tuple]) > 0):\n",
        "                        flows[flow_tuple].append(packet)\n",
        "                else:\n",
        "                    flows[flow_tuple] = [packet]\n",
        "        for flow_tuple, packets in flows.items():\n",
        "            X.append(extract_features(flows[flow_tuple]))\n",
        "            y.append(label)\n",
        "    for i in range(len(X)):\n",
        "        X[i].append(y[i])\n",
        "    file_name = \"/content/drive/MyDrive/HorusEye/DataSets/Dataplane/\"+name+\".csv\"\n",
        "    with open(file_name, 'w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(features)\n",
        "        writer.writerows(X)\n",
        "\n",
        "# for i in os.listdir(\"/content/drive/MyDrive/HorusEye/DataSets/HorusEye_Pcap/Pcap/Normal\"):\n",
        "#       flow = \"/content/drive/MyDrive/HorusEye/DataSets/HorusEye_Pcap/Pcap/Normal/\" + i\n",
        "#       name = \"Normal/\"+i\n",
        "#       extract_flows(flow,0,name)\n",
        "\n",
        "# for i in os.listdir(\"/content/drive/MyDrive/HorusEye/DataSets/HorusEye_Pcap/Pcap/Anomaly\"):\n",
        "#       flow = \"/content/drive/MyDrive/HorusEye/DataSets/HorusEye_Pcap/Pcap/Anomaly/\" + i\n",
        "#       name = \"Anomaly/\"+i\n",
        "#       extract_flows(flow,1,name)\n",
        "\n",
        "# for i in os.listdir(\"/content/drive/MyDrive/HorusEye/DataSets/HorusEye_Pcap/Pcap/robust/low_rate/tcp_ddos_0.01\"):\n",
        "#       flow = \"/content/drive/MyDrive/HorusEye/DataSets/HorusEye_Pcap/Pcap/robust/low_rate/tcp_ddos_0.01/\" + i\n",
        "#       name = \"robust/\"+i\n",
        "#       print(flow)\n",
        "#       print(name)\n",
        "#       extract_flows(flow,1,name)\n"
      ],
      "metadata": {
        "id": "n865TMwbA34z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}